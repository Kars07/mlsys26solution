# Blackwell Sparse Attention: Async Gather & Double Buffering

## üöÄ The Challenge
Unlike the "Indexer" (which processes dense pages), the **Attention** phase is dominated by **Random Memory Access**. We must fetch 2048 specific tokens from a 100GB+ KV cache, scattered randomly in memory. This is the classic "Gather" bottleneck.

## üß† Technical Architecture

### 1. Async Copy Pipeline (`cp.async`)
Instead of stopping the compute threads to fetch memory, we use the NVIDIA Ampere/Blackwell **Async Copy Engines**.
* **Mechanism:** `__pipeline_memcpy_async(dst, src, 16)`
* **Impact:** Memory fetches happen *in the background*. The Tensor Cores never stop computing.

### 2. Double Buffering (Ping-Pong)
We divide the Shared Memory (L1) into two stages (Stage 0 and Stage 1).
* **Phase A:** Compute on Stage 0 (data loaded previously).
* **Phase B:** Simultaneously load Stage 1 (next tile) from global memory.
* **Result:** The memory latency is completely **hidden**. The kernel runs at the speed of math, not memory.

### 3. Warp Specialization
We assign work to Warps (groups of 32 threads) rather than dividing by block.
* **Assignment:** 1 Warp = 2 Attention Heads.
* **Benefit:** Zero synchronization between warps. Each warp runs its own race to the finish line.

### 4. Register Compaction
* **Problem:** Storing 512 dimensions per thread ($512 \times 2\text{B} = 1\text{KB}$) caused register spilling to slow local memory.
* **Fix:** We lowered the "Items Per Thread" to 16 ($16 \times 2\text{B} = 32\text{B}$), forcing the compiler to keep data in the fast register file.

## üèÜ Performance
* **Speedup:** 176x vs PyTorch.
* **Latency:** 0.87ms (Flat scaling up to Batch 64).
